## 1.1: 데이터 통합 수행
다양한 데이터 수집 패턴 / 스트리밍 데이터와 배치 데이터 수집을 이해

### 데이터 엔지니어링 수명 주기
- 데이터 생성, 저장, 수집, 변형 및 제공 단계
1. 생성 :  데이터의 특성은 무엇입니까? 소스 시스템에서 데이터가 유지되는 방식은 무엇입니까? 중복 데이터가 존재합니까? 해당 소스 데이터의 스키마는 무엇이고 스키마는 변경됩니까?
2. 저장 : 어떤 스토리지를 사용할 것인지
3. 수집 : 수집되는 데이터의 사용 사례는 무엇입니까? 이 데이터는 수집된 후 어디로 이동합니까? 데이터가 수집되는 빈도 및 볼륨은 어떻게 됩니까? 데이터의 형식은 무엇입니까? 등입니다.
	- 데이터 통합 개념 : 배치 또는 스트리밍과 푸시 또는 풀
	- 재처리 : 실패 혹은 데이터 변경 혹은 업데이트 진행시 재처리 필요
		- 다양한 시뮬레이션 시나리오 및 재처리 시나리오를 사용하여 데이터 수집 파이프라인의 재처리를 테스트하여 멱등성(계속 반복해도 일관된 결과 도출)을 검증하고 데이터 손실, 중복 또는 무결성 문제 없이 일관되게 데이터를 재처리하는 것이 모범 사례
		- 파이프라인이 데이터 중복 또는 불일치 없이 동일한 데이터를 여러 번 처리하는 데 도움이 됩니다. 또한 파이프라인 내에 데이터 처리의 진행 상황 및 상태를 추적하여 마지막으로 성공적으로 처리된 데이터 포인트를 결정하는 체크포인트 메커니즘을 구성합니다. 그러면 오류 또는 중단이 발생할 경우 파이프라인이 해당 포인트부터 처리를 재개
	- 데이터 버전 관리 
		- 원시 또는 중간 데이터를 Amazon S3와 같은 확장 가능한 내구적 스토리지 서비스에 저장 -> 규정 준수 요구 사항, 데이터 거버넌스 정책 및 다시 재생을 위해 일정 기간 동안 데이터를 보존 
		- 데이터 수집 파이프라인 내에 로깅 및 모니터링을 추가하여 진행 상황을 추적, 실패 식별, 파이프라인 동작을 분석하기 위한 관련 로그, 오류 및 지표를 캡처
		- AWS CloudFormation 또는 AWS Cloud Development Kit (AWS CDK) 같은 코드형 인프라 도구를 사용하여 데이터 수집 파이프라인의 배포 및 구성을 자동화 (IaC)
	- 수집 예시
		- 애플리케이션이 S3 데이터 레이크에 기록 (TSV 파일 형식)
		- Athena에서 표준 SQL을 사용하여 데이터를 쿼리 및 분석
		- 혹은 TSV 파일을 Parquet와 같이 열기반 파일 형식으로 변환할 수 있음
		- 해당 데이터의 하위 집합이 필요하다면 Redshift에 삽입하고 Redshift Spectrum을 사용해 데이터 레이크 내 웨어하우스 쿼리를 결합할 수 있음
		- 새 데이터가 데이터 레이크에 오면 AWS Glue를 사용해서 테이블 업데이트
- 용도별 서비스
	- S3 : 데이터 레이크
	- Redshift (Spectrum) 데이터 웨어하우스, 레이크 하우스
	- EMR : 빅데이터 처리
	- Kinesis, MSK : 실시간 분석
	- OpenSearch : 모니터링, 로그 분석 등

### 데이터 별 처리 방식 및 서비스
- 데이터의 5V : 다양성 볼륨 속도 정확성 유효성 가치
	- 다양성 : 데이터 유형
	- 볼륨 : 데이터 양(제한 없음). S3 , EBS, Redshift, DynamoDB
	- 속도 : 소스부터 대상까지의 수집 처리 속도. Kinesis, MSK, Opensearch, Lambda
	- 정확성, 유효성 : 데이터 품질 .. 가치 
- 트랜잭션 데이터 : 한 번에 완전히 처리되어야 하는 (트랜잭션) 정보
	- 서비스 : DynamoDB, RDS
	- 이관 : AWS DMS
	- 스테이트풀 및 스테이트리스 : 기억 vs 새롭게 시작
- 스트리밍 데이터 : 실시간 수집. 소용량 이벤트를 대규모로 읽기
	- 서비스 : MSK, Kinesis
	- 이관 : Data Firehose
	- 성능 조정을 충족하려면..? -> 스트림 지표를 모니터링하고 파티션 키 전략등을 사용해 핫 샤드 방지
- 다른 서비스 : AppFlow / Transfer Family / DataSync / Snow Family
- 배치 데이터 수집
	- 즉시 처리할 필요 없는 콜드 데이터
	- EMR (Hadoop / Spark) - JDBC로 RDB에 연결해 데이터 로드 (AWS Glue)

