
## 1.2: 데이터 변환 및 처리
특정 요구 사항을 충족하는 ETL 파이프라인을 생성
비정형 및 정형 데이터의 볼륨, 속도, 다양성을 다루는 방법
클라우드 컴퓨팅 및 분산 컴퓨팅을 위해 데이터를 설계, 변환, 처리하는 방법

### 쿼리 및 모델링
- 개념, 논리, 물리, 정규화
- OLTP (온라인 트랜잭션 처리) : 데이터 최신상태에 중점. 주문 처리, 결제 처리, 고객 데이터 관리 등.
- OLAP (온라인 분석 처리) : 복잡한 데이터 분석 및 보고에 최적화. 고객 행동 예측, 추세 분석 등.

### 데이터 변환
- 올바른 유형으로 매핑
- 데이터 스키마 변경 및 정규화 적용
- 배치 데이터 변환
- 분산 컴퓨팅을 사용한 변환 : Lambda, Amazon EMR, AWS Glue, Amazon Redshift
- 병렬 컴퓨팅을 이용한 변환 : Amazon EMR, AWS Batch, AWS Step Functions
- 서버리스를 이용한 변환 : Glue
- 빅데이터 처리 도구 : Spark, Hive, Apache Hudi, Apache HBase, Presto, Pig (AWS EMR)
- Flink : SQL 명령을 사용하여 기본 데이터 변환 수행 -> EMR, Glue : 더 복잡한 데이터 변환 수행

### Spark 사용해 데이터 처리하는 방법 
1. EMR 클러스터 시작
2. Spark 설정 및 프로그램 만들기 (구성파일 수정하여 워크로드 요구사항 기반으로 메모리 할당)
3. 클러스터에 보내 실행. 모니터링과 최적화
4. 결과 저장하기
-  Amazon EMR의 경우 Spark를 실행하려는 클러스터에 특정 구성이 필요하므로 데이터 처리에 하둡 에코시스템을 사용할 때 이용하는 것이 좋음

|**특징**|**AWS Glue**|**AWS EMR**|
|---|---|---|
|**서비스 유형**|서버리스 ETL 서비스|관리형 빅데이터 플랫폼|
|**주요 사용 사례**|데이터 정리, 보강, 이동 등 간단한 ETL 작업|대규모 분산 데이터 처리, 머신 러닝, 실시간 데이터 스트리밍 분석|
|**지원 기술 및 프레임워크**|내장된 ETL 기능 및 AWS Glue Data Catalog 활용|Apache Spark, Hive, Pig, HBase, Presto 등 다양한 오픈소스 빅데이터 툴 지원|
|**유연성**|제한적 (Glue의 기본 제공 기능에 의존)|높은 유연성 (클러스터 구성 및 다양한 인프라 선택 가능)|
|**관리 요구사항**|클러스터 관리 불필요 (완전관리형)|클러스터 관리 필요|
|**비용 모델**|사용량 기반 과금 (PAYG)|인프라 및 클러스터 구성에 따라 비용 조정 가능|
|**확장성**|제한적 (Glue 워커 유형에 따라 메모리 제약 있음)|페타바이트 수준 확장 가능, 다양한 EC2 인스턴스 유형 활용 가능|
|**데이터 형식**|구조화된 데이터에 적합 (CSV, JSON 등)|반구조화 및 비정형 데이터 처리 가능|
|**실시간 처리 능력**|제한적 (100초 처리 창 제한 등)|실시간 스트리밍 분석 지원|
|**데이터 카탈로그 통합**|AWS Glue Data Catalog를 기본적으로 사용|Glue Data Catalog와 통합 가능|

### 예1) 키바나 대시보드 데이터 시각화
1. kinesis 설치 후 kinesis data firehose로 지속적으로 전송하도록 구성
2. kinesis data firehose가 s3 버킷으로 스트림의 데이터를 전송하도록 구성
3. glue를 이용해 카탈로그화 (데이터 레이크 솔루션의 일부로 사용)
4. firehose 처리를 위해 스트림의 데이터를 flink로 전송하도록 구성
5. flink를 이용해 스트림 데이터 집계 후 다른 firehose로 보냄
6. firehose에서 사용자 보고서에 사용하기 위해 인덱싱될 데이터를 opensearch로 보냄
7. kibana가 opensearch에 연결해 사용자를 위한 로그 레코드의 시각화 구축

### 예2) 과거 트랜잭션을 판매 실적 보고서 데이터와 조인
1. Redshift 클러스터로 데이터 처리
2. Redshift SQL을 사용하여 테이블을 조인하고 Redshift Spectrum을 사용하여 S3에서 과거 트랜잭션 데이터를 위한 외부 테이블 생성

### 예3) 변환 실패 및 성능 문제 해결 방법
- 데이터 변환시의 AWS 서비스에서 생성된 로그 확인
- 데이터 품질 및 무결성 확인
- 소스 데이터에서 변환 로직을 검증해 매핑, 필터, 집계 및 계산이 모두 바른지 확인

### 예4) 다른 시스템에서 데이터를 사용할 수 있도록 API 생성
- 데이터 저장 위치 파악
- API gateway를 사용하여 API 생성
- 보안과 인증 설정 : API Gateway, Lambda 함수에 대한 액세스 권한 제어로 보호
- 캐싱 및 성능 최적화 : ElastiCache 또는 API Gateway 내장 캐싱기능 사용해 응답 캐시하고 지연시간 줄일 수 있음.

